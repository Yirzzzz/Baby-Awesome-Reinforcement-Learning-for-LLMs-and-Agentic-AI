

# Baby Awesome Reinforcement Learning for LLMs and Agentic AI

![ChatGPT Image 2025Âπ¥9Êúà11Êó• 15_50_21](../../download/ChatGPT Image 2025Âπ¥9Êúà11Êó• 15_50_21.png)

> This is not a canonical "Awesome" list.  It‚Äôs **my learning log** of reinforcement learning for LLMs and agentic AI.  The repo curates papers, blogs, and implementations I read along the way.  Mistakes may occur ‚Äî corrections and PRs are always welcome! üôå If you want to contribute to this list, welcome to send me a pull request or contact me :)

[TOC]

## üìñ Foundations of LLM-RL

- **TRPO(2015)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/1502.05477)[![Code](https://img.shields.io/badge/Code-pytorch--trpo-black)](https://github.com/ikostrikov/pytorch-trpo)

- **PPO (2017)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/1707.06347)

- **InstructGPT (2023)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2203.02155)

> [!note]
>
> `InstructGPT` introduces a new training paradigm‚ÄîRLHF (Reinforcement Learning from Human Feedback) that aligns models with user intent rather than pure next-token likelihood. It train a preference-based **reward model (RM)** from pairwise human judgments, and  optimize the policy with **PPO** under a **KL penalty** to a reference model to preserve fluency while shifting behavior toward human-preferred responses.

==TOSTUDY==

- **DQN (2015)**
- **SAC (2018)** 
- **Books & Tutorials**:
  - *Reinforcement Learning: An Introduction* (Sutton & Barto)
  - [OpenAI Spinning Up](https://spinningup.openai.com/)



## ‚öñÔ∏è Preference Optimization (Critic-Free / Lightweight RL)

> These methods convert preference learning into a contrastive/classification objective and do not perform Online reinforcement learning  `online RL`, it means the policy is **trained while continuously interacting with an environment** to collect **fresh trajectories** generated by the **current** policy.

- **DPO (2023)  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2305.18290)[![Code](https://img.shields.io/badge/Code-pytorch--DPO-black)](https://github.com/ikostrikov/pytorch-trpo)**

==TOSTUDY==

- IPO (2023) ‚Äì Implicit Preference Optimization
- [SimPO (Meta, 2024)](https://arxiv.org/abs/2405.14734?utm_source=chatgpt.com) ‚Äì Simple Preference Optimization
- RLOO (2024) ‚Äì Leave-One-Out RL
- ORPO (2024) ‚Äì Odds Ratio Preference Optimization
- KTO (2024) ‚Äì Kahneman-Tversky Optimization
- [RRHF (2023)](https://arxiv.org/abs/2304.05302?utm_source=chatgpt.com) ‚Äì Rank Responses with Human Feedback
- **Implementations**:

  - [DPO repo (Eric Mitchell)](https://github.com/eric-mitchell/direct-preference-optimization)
  - [Alignment Handbook (Anthropic)](https://github.com/anthropics/rlhf)



## ü§ñ RLAIF (Reinforcement Learning from AI Feedback)

> These methods are data-labeling paradigm, replaceing human preference labels with **AI-generated judgments**. They can feed either **preference-optimization objectives (e.g., DPO/IPO/ORPO)** or **RLHF with PPO**. (`off-policy`)

- **Constitutional AI (2022)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2212.08073)
- **Self-Rewarding LLMs (2024)** [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2401.10020)
- **Blogs & Resources**:
  - [OpenAI Blog: AI Feedback for Alignment](https://openai.com/research/learning-from-ai-feedback)



## üßÆ Process Supervision & Verifiable Rewards

> Process Supervision provides **step-level signals** (correct/incorrect rationales, tool traces) instead of outcome-only labels.

- [Let‚Äôs Verify Step by Step (OpenAI, 2023)](https://arxiv.org/abs/2305.20050?utm_source=chatgpt.com) ‚Äì Process reward models (PRMs).
- RLVR (2024) ‚Äì Reinforcement Learning with Verifiable Rewards.
- Math-Shepherd (2024) ‚Äì Verifiable process supervision for math reasoning.



## üî• GRPO & Online RL for LLMs

- **GRPO (DeepSeek, 2025)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/pdf/2402.03300)

> [!note]
>
> `GRPO` is a **critic-free** method that derives group-normalized advantages from **relative rewards within each candidate group**, avoiding value-function training, which removes the need to train a separate value function by optimizing updates.

==TOSTUDY==

- **REINFORCE++ (2025)** [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2501.03262)

- **RLOO (2024)** [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2402.14740)



## üßë‚Äçüíª Agentic RL

- **Surveys & Overviews**:
  - [Agentic RL Survey (2025)](https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com)
- **Algorithms**:
  - [CHORD (2025)](https://arxiv.org/abs/2508.12800?utm_source=chatgpt.com) ‚Äì Harmonizing on- and off-policy RL
  - [ARPO (2025)](https://arxiv.org/abs/2507.19849?utm_source=chatgpt.com) ‚Äì Agentic Reinforced Policy Optimization
  - [Chain-of-Agents (2025)](https://arxiv.org/abs/2508.13167?utm_source=chatgpt.com) ‚Äì Multi-agent coordination
- **Benchmarks**:
  - WebArena ‚Äì Web environment benchmark
  - [AgentBench](https://arxiv.org/abs/2308.03688?utm_source=chatgpt.com)
- **Implementations**:
  - [VERL (Volcengine RL Toolkit)](https://verl.readthedocs.io/en/latest/start/agentic_rl.html?utm_source=chatgpt.com)
  - [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF?utm_source=chatgpt.com)



## üõ† Toolkits

- [HuggingFace TRL](https://github.com/huggingface/trl?utm_source=chatgpt.com)
- [trlx (CarperAI)](https://github.com/CarperAI/trlx)
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF?utm_source=chatgpt.com)
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)
- [VeRL (Volcengine)](https://verl.readthedocs.io/en/latest/start/agentic_rl.html?utm_source=chatgpt.com)