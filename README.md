

# Baby Awesome Reinforcement Learning for LLMs and Agentic AI

> ‚ö†Ô∏è Disclaimer: This is not a canonical "Awesome" list.  
> It‚Äôs  **my learning log** of reinforcement learning for LLMs and agentic AI.  
> The repo curates papers, blogs, and implementations I read along the way.  
> Mistakes may occur ‚Äî corrections and PRs are always welcome! üôå

![ChatGPT Image 2025Âπ¥9Êúà11Êó• 15_50_21](../../download/ChatGPT Image 2025Âπ¥9Êúà11Êó• 15_50_21.png)

[TOC]

## üìñ Foundations of LLM-RL

- **TRPO(2015)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/1502.05477)[![Code](https://img.shields.io/badge/Code-pytorch--trpo-black)](https://github.com/ikostrikov/pytorch-trpo)

- **PPO (2017)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/1707.06347)

  

- **DQN (2015)**

- **SAC (2018)** 

- **Books & Tutorials**:

  - *Reinforcement Learning: An Introduction* (Sutton & Barto)
  - [OpenAI Spinning Up](https://spinningup.openai.com/)

------

## ü§ù RLHF (Reinforcement Learning with Human Feedback)

- **InstructGPT**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2203.02155)
- **Implementations**:
  - [HuggingFace TRL](https://github.com/huggingface/trl?utm_source=chatgpt.com)
  - [trlx (CarperAI)](https://github.com/CarperAI/trlx)
  - [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)
- **Blogs & Guides**:
  - Lil‚ÄôLog: RLHF Explained
  - HuggingFace Blog on RLHF

------

## ‚öñÔ∏è Preference Optimization (Critic-Free / Lightweight RL)

> These methods convert preference learning into a contrastive/classification objective and do not perform Online reinforcement learning  `online RL`, it means the policy is **trained while continuously interacting with an environment** to collect **fresh trajectories** generated by the **current** policy.

- **DPO (2023)  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/abs/2305.18290)[![Code](https://img.shields.io/badge/Code-pytorch--DPO-black)](https://github.com/ikostrikov/pytorch-trpo)**

  

  - [DPO (Anthropic, 2023)](https://arxiv.org/abs/2305.18290?utm_source=chatgpt.com) ‚Äì Direct Preference Optimization
  - IPO (2023) ‚Äì Implicit Preference Optimization
  - [SimPO (Meta, 2024)](https://arxiv.org/abs/2405.14734?utm_source=chatgpt.com) ‚Äì Simple Preference Optimization
  - RLOO (2024) ‚Äì Leave-One-Out RL
  - ORPO (2024) ‚Äì Odds Ratio Preference Optimization
  - KTO (2024) ‚Äì Kahneman-Tversky Optimization
  - [RRHF (2023)](https://arxiv.org/abs/2304.05302?utm_source=chatgpt.com) ‚Äì Rank Responses with Human Feedback

- **Implementations**:

  - [DPO repo (Eric Mitchell)](https://github.com/eric-mitchell/direct-preference-optimization)
  - [Alignment Handbook (Anthropic)](https://github.com/anthropics/rlhf)

------

## ü§ñ RLAIF (Reinforcement Learning from AI Feedback)

- **Papers**: 
  - [Constitutional AI (Anthropic, 2022)](https://arxiv.org/abs/2212.08073?utm_source=chatgpt.com)
  - Self-Rewarding LLMs (2023)
- **Blogs & Resources**:
  - [OpenAI Blog: AI Feedback for Alignment](https://openai.com/research/learning-from-ai-feedback)

------

## üßÆ Process Supervision & Verifiable Rewards

- [Let‚Äôs Verify Step by Step (OpenAI, 2023)](https://arxiv.org/abs/2305.20050?utm_source=chatgpt.com) ‚Äì Process reward models (PRMs).
- RLVR (2024) ‚Äì Reinforcement Learning with Verifiable Rewards.
- Math-Shepherd (2024) ‚Äì Verifiable process supervision for math reasoning.

------

## üî• GRPO & Online RL for LLMs

- **GRPO (DeepSeek, 2025)**  [![Paper](https://img.shields.io/badge/Paper-arXiv-blue)](https://arxiv.org/pdf/2402.03300)



- REINFORCE++ (2024) ‚Äì Stable critic-free REINFORCE variant.

- RLOO (2024) ‚Äì Efficient leave-one-out baseline.

------

## üßë‚Äçüíª Agentic RL

- **Surveys & Overviews**:
  - [Agentic RL Survey (2025)](https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com)
- **Algorithms**:
  - [CHORD (2025)](https://arxiv.org/abs/2508.12800?utm_source=chatgpt.com) ‚Äì Harmonizing on- and off-policy RL
  - [ARPO (2025)](https://arxiv.org/abs/2507.19849?utm_source=chatgpt.com) ‚Äì Agentic Reinforced Policy Optimization
  - [Chain-of-Agents (2025)](https://arxiv.org/abs/2508.13167?utm_source=chatgpt.com) ‚Äì Multi-agent coordination
- **Benchmarks**:
  - WebArena ‚Äì Web environment benchmark
  - [AgentBench](https://arxiv.org/abs/2308.03688?utm_source=chatgpt.com)
- **Implementations**:
  - [VERL (Volcengine RL Toolkit)](https://verl.readthedocs.io/en/latest/start/agentic_rl.html?utm_source=chatgpt.com)
  - [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF?utm_source=chatgpt.com)

------

## üõ† Toolkits

- [HuggingFace TRL](https://github.com/huggingface/trl?utm_source=chatgpt.com)
- [trlx (CarperAI)](https://github.com/CarperAI/trlx)
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF?utm_source=chatgpt.com)
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)
- [VeRL (Volcengine)](https://verl.readthedocs.io/en/latest/start/agentic_rl.html?utm_source=chatgpt.com)
